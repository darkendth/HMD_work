{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, json, random, logging, time, socket\n",
    "import numpy as np\n",
    "from itertools import cycle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from matplotlib import gridspec\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.gaussian_process.kernels import RationalQuadratic\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from sklearn.gaussian_process.kernels import ExpSineSquared\n",
    "from sklearn.gaussian_process.kernels import DotProduct\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from scipy import interp\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import manifold\n",
    "import matplotlib.pylab as pylab\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "malware_file = 'amd_malware_metadata/malware_post_pca_data.txt'\n",
    "\n",
    "with open(malware_file, 'r') as data_file:\n",
    "    mlw_data = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "malware = {}\n",
    "for m in mlw_data.keys():\n",
    "    for each_sample in mlw_data[m]:\n",
    "        if m not in malware:\n",
    "            malware[m] = np.array(each_sample)\n",
    "        else:\n",
    "            malware[m] = np.vstack((malware[m], each_sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "malware_fam = {}\n",
    "with open('fam_data/myth_label.txt', 'r') as jk:\n",
    "    for line in jk:\n",
    "        l = line.strip().split(\"\\t\")\n",
    "        if l[1].startswith(\"SINGLETON:\"):\n",
    "            l[1] = l[1].split(\":\")[0]\n",
    "        if l[0] in malware:\n",
    "            if l[1] not in malware_fam.keys():\n",
    "                malware_fam[l[1]] = malware[l[0]]\n",
    "            else:\n",
    "                malware_fam[l[1]] = np.vstack(\n",
    "                    (malware_fam[l[1]], malware[l[0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = []\n",
    "with open('fam_data/myth_label.txt', 'r') as jk:\n",
    "    for line in jk:\n",
    "        l = line.strip().split(\"\\t\")\n",
    "        if l[1].startswith(\"SINGLETON:\"):\n",
    "            l[1] = l[1].split(\":\")[0]\n",
    "        if l[1] not in tt:\n",
    "            tt.append(l[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(malware_fam.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_99792/236228987.py:14: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if benign_data == []:\n"
     ]
    }
   ],
   "source": [
    "benign_files = [\\\n",
    "        'amd_desktop_metadata/benign_post_pca_data.txt',\\\n",
    "        'amd_python_benign_metadata/python_benign_post_pca_data.txt'\n",
    "        ]\n",
    "malware_files = ['amd_malware_metadata/malware_post_pca_data.txt']\n",
    "\n",
    "benign_data = []\n",
    "for files in benign_files:\n",
    "    with open(files, 'r') as data_file:\n",
    "        json_data = json.load(data_file)\n",
    "\n",
    "    for bm in json_data.keys():\n",
    "        for each_sample in json_data[bm]:\n",
    "            if benign_data == []:\n",
    "                benign_data = np.array(each_sample) \n",
    "            else:\n",
    "                benign_data = np.vstack((benign_data, each_sample))\n",
    "\n",
    "    data_file.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45648, 12)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benign_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class learnAll:\n",
    "    def __init__(self, malware, benign):\n",
    "        self.malware_data = malware\n",
    "        self.benign_data = benign\n",
    "\n",
    "        malware_label = np.ones(len(malware))\n",
    "        benign_label = np.zeros(len(benign_data))\n",
    "        sample_size = min(len(benign_label), len(malware_label))\n",
    "        self.data = np.vstack((\n",
    "            random.sample(list(benign_data), sample_size),\n",
    "            random.sample(list(malware), sample_size)\n",
    "        ))\n",
    "        self.label = np.concatenate((\n",
    "            random.sample(list(benign_label), sample_size),\n",
    "            random.sample(list(malware_label), sample_size)\n",
    "        ))\n",
    "\n",
    "    def setup_train_test_split(self):\n",
    "        \"\"\"\n",
    "        Setup_train_test_split uses default split (scikit-learn split) to separate the \n",
    "        training and testing dataset.\n",
    "        \"\"\"\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.data, self.label, test_size=0.1,\n",
    "            random_state=random.randint(0, 2**32 - 1))\n",
    "\n",
    "    def setup_classifiers(self):\n",
    "        \"\"\"\n",
    "        setup_classifiers only generate the classifier class objects. nothing more.\n",
    "        \"\"\"\n",
    "        self.names = [\n",
    "            \"Nearest Neighbors\", \"Decision Tree\",\n",
    "            \"Random Forest\",\n",
    "            \"AdaBoost\", \"Naive Bayes\",\n",
    "            \"Neural Net\" \\\n",
    "            # \"Logistic Regression\",\\\n",
    "            # \"Linear SVM\", \"Rbf SVM\", \"Poly SVM\", \"Sigmoid SVM\"\\\n",
    "            # , \"Gaussian Process\"\n",
    "        ]\n",
    "        max_iterations = 1000\n",
    "        self.classifiers = [\n",
    "            KNeighborsClassifier(\n",
    "                n_neighbors=5, weights='uniform', algorithm='auto', n_jobs=-1),\n",
    "            DecisionTreeClassifier(\n",
    "                max_depth=100, min_samples_split=12, min_samples_leaf=12,\n",
    "                max_features=None,\n",
    "                random_state=int(round(time.time()))),\n",
    "            RandomForestClassifier(max_depth=100, min_samples_split=12,\n",
    "                                   min_samples_leaf=12,\n",
    "                                   n_estimators=100, max_features=None,\n",
    "                                   random_state=int(round(time.time()))),\n",
    "            AdaBoostClassifier(algorithm='SAMME.R', n_estimators=200,\n",
    "                               random_state=int(round(time.time()))),\n",
    "            GaussianNB(priors=[0.5, 0.5]),\n",
    "            MLPClassifier(hidden_layer_sizes=(100, 100, 100, 100),\n",
    "                          alpha=100, solver='lbfgs',\n",
    "                          max_iter=max_iterations,\n",
    "                          activation='tanh', tol=1e-5,\n",
    "                          warm_start=True) \\\n",
    "            # LogisticRegression(penalty='l2', tol=1e-4, C=1e2,\\\n",
    "            # fit_intercept=True, solver='lbfgs', \\\n",
    "            # class_weight='balanced', max_iter=max_iterations), \\\n",
    "            # SVC(kernel=\"linear\", C=1e2, tol=1e-4, max_iter=max_iterations,\\\n",
    "            # probability= True),\\\n",
    "            # SVC(kernel=\"rbf\", C=1e2, tol=1e-4, max_iter=max_iterations,\\\n",
    "            # probability=True, shrinking=True),\n",
    "            # SVC(kernel=\"poly\", C=1e2, degree=4, tol=1e-4,\\\n",
    "            # max_iter=max_iterations, probability=True),\\\n",
    "            # SVC(kernel=\"sigmoid\", C=1e2, gamma=1e-1, tol=1e-3, \\\n",
    "            # max_iter=max_iterations, probability=True, \\\n",
    "            # shrinking=True)#,\\\n",
    "            # GaussianProcessClassifier(1.0 * RBF(1.0), n_jobs=-1, \\\n",
    "            # copy_X_train=False, \\\n",
    "            # max_iter_predict=100, warm_start=False )\\\n",
    "        ]\n",
    "        # self.classifiers = [ \\\n",
    "        #                 LogisticRegression(penalty='l2', tol=1e-9, C=1,\\\n",
    "        #                   fit_intercept=True, solver='lbfgs', \\\n",
    "        #                   class_weight='balanced', max_iter=max_iterations*100, n_jobs=-1) \\\n",
    "        #                 ]\n",
    "\n",
    "    def text_class_report(self, names,fam):\n",
    "        \"\"\"\n",
    "        Text_class_report for experiment log files. text report updates the self.report \n",
    "        to log the results in the terminal. The results include precision, recall, f1_score\n",
    "        and support. The results are dumped into classification report. The labels are 0 and 1.\n",
    "        \"\"\"\n",
    "        # results\n",
    "        precision, recall, f1_score, support = \\\n",
    "            precision_recall_fscore_support(self.y_test, self.prediction,\n",
    "                                            beta=1.0, average='binary', labels=[0, 1], pos_label=0)\n",
    "        self.report[names] = {}\n",
    "        self.report[names]['Accuracy'] = accuracy_score(self.y_test, self.prediction)\n",
    "        self.report[names]['precision'] = precision\n",
    "        self.report[names]['recall'] = recall\n",
    "        self.report[names]['f1_score'] = f1_score\n",
    "        self.report[names]['support'] = support\n",
    "        self.report[names]['param'] = \\\n",
    "            self.classifiers[self.names.index(names)].get_params()\n",
    "\n",
    "        with open(fam+'_classification_report.txt', 'w') as outfile:\n",
    "            json.dump(self.report, outfile, indent=4)\n",
    "        outfile.close()\n",
    "        # print(self.report)\n",
    "\n",
    "    def roc_curve_report(self, name):\n",
    "        \"\"\"\n",
    "        Use fpr and tpr information to record the roc information.\n",
    "        \"\"\"\n",
    "        fpr, tpr, _ = roc_curve(\n",
    "            self.y_test, self.predict_proba[:, 0], pos_label=0)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        self.report[name]['fpr'] = list(fpr)\n",
    "        self.report[name]['tpr'] = list(tpr)\n",
    "        self.report[name]['roc_auc'] = roc_auc\n",
    "\n",
    "        with open('classification_report.txt', 'w') as outfile:\n",
    "            json.dump(self.report, outfile, indent=2)\n",
    "        outfile.close()\n",
    "\n",
    "    def run(self,fam):\n",
    "        \"\"\"\n",
    "        run function has the minimum experimental setup. It uses the basic training and \n",
    "        testing split.\n",
    "        \"\"\"\n",
    "\n",
    "        self.setup_train_test_split()\n",
    "        self.setup_classifiers()\n",
    "        self.report = {}\n",
    "        for names, clf in zip(self.names, self.classifiers):\n",
    "            print(names + \" in process ....\")\n",
    "            # train\n",
    "            clf.fit(self.X_train, self.y_train)\n",
    "            # if names == 'Decision Tree':\n",
    "            #     tree.export_graphviz(clf, out_file=self.result_path + 'tree.dot')\n",
    "            # test\n",
    "\n",
    "            self.prediction = clf.predict(self.X_test)\n",
    "            self.text_class_report(names,fam)\n",
    "\n",
    "            self.predict_proba = clf.predict_proba(self.X_test)\n",
    "            self.roc_curve_report(names)\n",
    "\n",
    "            print(names + \" testing completed!\")\n",
    "\n",
    "    def cv_text_class_report(self, names):\n",
    "        \"\"\"\n",
    "        cv_text_class_report generates logs for cross validation results.\n",
    "        \"\"\"\n",
    "        chunk_length = len(self.label) / 10\n",
    "        self.report[names] = {}\n",
    "        for idx in range(10):\n",
    "            precision, recall, f1_score, support = \\\n",
    "                precision_recall_fscore_support(\n",
    "                    self.label[idx * chunk_length: (idx + 1) * chunk_length],\n",
    "                    self.prediction[idx *\n",
    "                                    chunk_length: (idx + 1) * chunk_length],\n",
    "                    beta=1.0, average='binary', labels=[0, 1], pos_label=0)\n",
    "            self.report[names][idx] = {}\n",
    "            self.report[names][idx]['precision'] = precision\n",
    "            self.report[names][idx]['recall'] = recall\n",
    "            self.report[names][idx]['f1_score'] = f1_score\n",
    "            self.report[names][idx]['support'] = support\n",
    "\n",
    "        # with open(self.result_path + 'cv_classification_report.txt', 'w') as outfile:\n",
    "        #     json.dump(self.report, outfile, indent=2)\n",
    "        # outfile.close()\n",
    "        print(self.report)\n",
    "\n",
    "    def classic_cross_validation(self):\n",
    "        \"\"\"\n",
    "        classic_cross_validation runs cross validation with ALL the data in the feature\n",
    "        vectors. It does NOT separate feature vectors between training and testing dataset.\n",
    "        \"\"\"\n",
    "        print('Start to cross validate ...')\n",
    "        self.setup_classifiers()\n",
    "        self.report = {}\n",
    "        for names, clf in zip(self.names, self.classifiers):\n",
    "            self.logger.info(names + \" in process ....\")\n",
    "            # test\n",
    "            zipped_list = zip(self.data, self.label)\n",
    "            random.shuffle(zipped_list)\n",
    "            self.data, self.label = zip(*zipped_list)\n",
    "            self.data = list(self.data)\n",
    "            self.label = list(self.label)\n",
    "            self.prediction = cross_val_predict(\n",
    "                clf, self.data, self.label, cv=10)\n",
    "            self.cv_text_class_report(names)\n",
    "\n",
    "            print(names + \" testing completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learnAll(malware_fam['SINGLETON'], benign_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors in process ....\n",
      "Nearest Neighbors testing completed!\n",
      "Decision Tree in process ....\n",
      "Decision Tree testing completed!\n",
      "Random Forest in process ....\n",
      "Random Forest testing completed!\n",
      "AdaBoost in process ....\n",
      "AdaBoost testing completed!\n",
      "Naive Bayes in process ....\n",
      "Naive Bayes testing completed!\n",
      "Neural Net in process ....\n",
      "Neural Net testing completed!\n"
     ]
    }
   ],
   "source": [
    "learn.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.cv_text_class_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23300\n",
      "vobfus\n",
      "Nearest Neighbors in process ....\n",
      "Nearest Neighbors testing completed!\n",
      "Decision Tree in process ....\n",
      "Decision Tree testing completed!\n",
      "Random Forest in process ....\n",
      "Random Forest testing completed!\n",
      "AdaBoost in process ....\n",
      "AdaBoost testing completed!\n",
      "Naive Bayes in process ....\n",
      "Naive Bayes testing completed!\n",
      "Neural Net in process ....\n",
      "Neural Net testing completed!\n",
      "1276\n",
      "SINGLETON\n",
      "Nearest Neighbors in process ....\n",
      "Nearest Neighbors testing completed!\n",
      "Decision Tree in process ....\n",
      "Decision Tree testing completed!\n",
      "Random Forest in process ....\n",
      "Random Forest testing completed!\n",
      "AdaBoost in process ....\n",
      "AdaBoost testing completed!\n",
      "Naive Bayes in process ....\n",
      "Naive Bayes testing completed!\n",
      "Neural Net in process ....\n",
      "Neural Net testing completed!\n",
      "1593\n",
      "beebone\n",
      "Nearest Neighbors in process ....\n",
      "Nearest Neighbors testing completed!\n",
      "Decision Tree in process ....\n",
      "Decision Tree testing completed!\n",
      "Random Forest in process ....\n",
      "Random Forest testing completed!\n",
      "AdaBoost in process ....\n",
      "AdaBoost testing completed!\n",
      "Naive Bayes in process ....\n",
      "Naive Bayes testing completed!\n",
      "Neural Net in process ....\n",
      "Neural Net testing completed!\n",
      "923\n",
      "smartfortress\n",
      "Nearest Neighbors in process ....\n",
      "Nearest Neighbors testing completed!\n",
      "Decision Tree in process ....\n",
      "Decision Tree testing completed!\n",
      "Random Forest in process ....\n",
      "Random Forest testing completed!\n",
      "AdaBoost in process ....\n",
      "AdaBoost testing completed!\n",
      "Naive Bayes in process ....\n",
      "Naive Bayes testing completed!\n",
      "Neural Net in process ....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/null_void/Music/venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Net testing completed!\n",
      "762\n",
      "zbot\n",
      "Nearest Neighbors in process ....\n",
      "Nearest Neighbors testing completed!\n",
      "Decision Tree in process ....\n",
      "Decision Tree testing completed!\n",
      "Random Forest in process ....\n",
      "Random Forest testing completed!\n",
      "AdaBoost in process ....\n",
      "AdaBoost testing completed!\n",
      "Naive Bayes in process ....\n",
      "Naive Bayes testing completed!\n",
      "Neural Net in process ....\n",
      "Neural Net testing completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/null_void/Music/venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "for fam in malware_fam:\n",
    "    if len(malware_fam[fam]) > 500:\n",
    "        print(len(malware_fam[fam]))\n",
    "        print(fam)\n",
    "        learn = learnAll(malware_fam[fam], benign_data)\n",
    "        learn.run(fam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61967/2171748639.py:33: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if self.benign_data == []:\n",
      "/tmp/ipykernel_61967/2171748639.py:46: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if self.malware_data == []:\n"
     ]
    }
   ],
   "source": [
    "malware_file = 'amd_malware_metadata/malware_post_pca_data.txt'\n",
    "\n",
    "with open(malware_file, 'r') as data_file:\n",
    "    mlw_data = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecilizedHMD:\n",
    "\tdef __init__ (self, logger, data_path_prefix='./'):\n",
    "\t\t\"\"\"\n",
    "\t\tinit function for initiating the members\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tself.benign_data    = []\n",
    "\t\tself.malware_data   = []\n",
    "\t\tself.benign_dict    = {}\n",
    "\t\tself.malware_dict   = {}\n",
    "\t\tself.result_path    = './'\n",
    "\n",
    "\t\t# setup seed\n",
    "\t\trandom.seed(os.urandom(100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
